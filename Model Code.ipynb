{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a0417d8-5db7-4625-99f9-2f38fd59d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from vit_pytorch import ViT\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363b1971-5e28-4850-b4a5-6ef31f014a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full DataFrame\n",
    "with open('decomposed_data.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "label_map = {'interictal': 0, 'preictal': 1}\n",
    "df['label'] = df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f92a35d-cc80-4692-b87d-b105ce874fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tensors have the same shape:\n",
      "(119, 65, 10)    7666\n",
      "Name: reduced_spectrogram, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Suppose your DataFrame is df and the tensor column is 'tensor_column'\n",
    "shapes = df['reduced_spectrogram'].apply(lambda x: x.shape)\n",
    "\n",
    "# Check if all shapes are the same\n",
    "all_same_shape = shapes.nunique() == 1\n",
    "\n",
    "print(\"All tensors have the same shape:\" if all_same_shape else \"Tensors have varying shapes.\")\n",
    "print(shapes.value_counts())  # optional: see the distribution of shapes\n",
    "spectrogram_shape = shapes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "378a85ee-5b58-4059-8648-aae9cc45c10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6835\n",
       "1     831\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edd26675-ba11-43d5-92e4-37d1a0c914d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts after balancing:\n",
      "1    831\n",
      "0    831\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate the two classes\n",
    "class_0 = df[df['label'] == 0]\n",
    "class_1 = df[df['label'] == 1]\n",
    "\n",
    "# Randomly sample from the majority class\n",
    "class_0_downsampled = class_0.sample(n=len(class_1), random_state=42)\n",
    "\n",
    "# Combine the downsampled class with the minority class\n",
    "df_balanced = pd.concat([class_0_downsampled, class_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Label counts after balancing:\")\n",
    "print(df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ada8e4fa-af53-4d47-8163-fdce703231b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (119, 65, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check the first entry\n",
    "sample_tensor = df_balanced.iloc[0]['reduced_spectrogram']\n",
    "print(\"Type:\", type(sample_tensor))\n",
    "print(\"Shape:\", sample_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5a3acf5-1b89-4975-8ca9-326ee127aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before permute: torch.Size([119, 65, 10])\n",
      "After permute: torch.Size([10, 119, 65])\n"
     ]
    }
   ],
   "source": [
    "# Convert and permute the dimensions\n",
    "tensor = torch.tensor(sample_tensor, dtype=torch.float32)\n",
    "print(\"Before permute:\", tensor.shape)\n",
    "\n",
    "tensor = tensor.permute(2, 0, 1)  # from (119, 65, 10) to (10, 119, 65)\n",
    "print(\"After permute:\", tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6aa26336-abe9-432a-88de-cc0b21280469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define resize transform\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((119, 63))  # Resize to dimensions divisible by patch size (7)\n",
    "])\n",
    "\n",
    "# Now apply it to the dataset\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.data = df\n",
    "        self.transform = transform\n",
    "        print(f\"Initialized dataset with {len(df)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = self.data.iloc[idx]['reduced_spectrogram']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Check if the tensor is an ndarray and convert it to a PyTorch tensor\n",
    "        if isinstance(tensor, np.ndarray):\n",
    "            tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "\n",
    "        # Permute the tensor to (channels, height, width)\n",
    "        tensor = tensor.permute(2, 0, 1)\n",
    "        \n",
    "        # Apply transformation (e.g., resizing) if provided\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        #print(f\"Sample {idx}: tensor.shape = {tensor.shape}, label = {label.item()}\")\n",
    "\n",
    "        return tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "79b60150-4bb7-4cde-9ac9-69a7526794e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 1662 samples\n",
      "Batch 0: x shape = torch.Size([16, 10, 119, 63]), y shape = torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset with resizing transform\n",
    "dataset = SpectrogramDataset(df_balanced, transform=resize_transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Get a batch and check its shape\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}: x shape = {x_batch.shape}, y shape = {y_batch.shape}\")\n",
    "    break  # Only look at one batch for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1fedbd98-12bc-4855-9cd2-707dd3a46489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=(10, 119, 63), patch_size=7, num_classes=2, embed_dim=256, num_heads=8, num_layers=6):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Calculate number of patches (height, width divided by patch size)\n",
    "        self.num_patches = (img_size[1] // patch_size) * (img_size[2] // patch_size)\n",
    "        \n",
    "        # Patch embedding layer\n",
    "        self.patch_embed = nn.Conv2d(in_channels=img_size[0], out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # MLP head for classification\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Pooling and classification head\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361537a2-0485-45dd-9af8-340ec76c4454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8506, Accuracy: 50.72%\n",
      "Epoch [2/50], Loss: 0.7302, Accuracy: 49.04%\n",
      "Epoch [3/50], Loss: 0.7005, Accuracy: 49.52%\n",
      "Epoch [4/50], Loss: 0.6931, Accuracy: 52.41%\n",
      "Epoch [5/50], Loss: 0.6943, Accuracy: 49.52%\n",
      "Epoch [6/50], Loss: 0.6939, Accuracy: 49.94%\n",
      "Epoch [7/50], Loss: 0.6958, Accuracy: 50.66%\n",
      "Epoch [8/50], Loss: 0.6961, Accuracy: 49.04%\n",
      "Epoch [9/50], Loss: 0.6951, Accuracy: 48.80%\n",
      "Epoch [10/50], Loss: 0.6942, Accuracy: 49.58%\n",
      "Epoch [11/50], Loss: 0.6937, Accuracy: 49.04%\n",
      "Epoch [12/50], Loss: 0.6941, Accuracy: 49.28%\n",
      "Epoch [13/50], Loss: 0.6937, Accuracy: 50.48%\n",
      "Epoch [14/50], Loss: 0.6953, Accuracy: 48.80%\n",
      "Epoch [15/50], Loss: 0.6938, Accuracy: 50.84%\n",
      "Epoch [16/50], Loss: 0.6934, Accuracy: 50.24%\n",
      "Epoch [17/50], Loss: 0.6948, Accuracy: 50.00%\n",
      "Epoch [18/50], Loss: 0.6940, Accuracy: 48.68%\n"
     ]
    }
   ],
   "source": [
    "# Set device to CPU if CUDA is unavailable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = VisionTransformer(img_size=(10, 119, 63), patch_size=7, num_classes=2, embed_dim=256, num_heads=16, num_layers=6)\n",
    "model = model.to(device)  # Move model to CPU or GPU\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192afad-6ac7-485d-9196-c163a53b277b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
